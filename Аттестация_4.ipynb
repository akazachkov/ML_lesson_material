{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Использовать модель и датасет из предыдущих домашних заданий или промежуточной аттестации и построить полноценный Pipeline для обработки и подготовки данных к моделированию.\n",
    "\n",
    "Цель:\n",
    "Автоматизировать процесс подготовки данных для улучшения качества моделей и упрощения этапов предобработки.\n",
    "\n",
    "Рекомендуемые шаги:\n",
    "1. Загрузка и анализ данных - определите типы данных и распределение признаков.\n",
    "\n",
    "2. Обработка пропущенных данных - используйте SimpleImputer или IterativeImputer для заполнения пропусков.\n",
    "\n",
    "3. Кодирование категориальных данных - примените OneHotEncoder или TargetEncoder.\n",
    "\n",
    "4. Масштабирование данных - используйте StandardScaler или RobustScaler.\n",
    "\n",
    "5. Построение модели - примените RandomForest или LightGBM как пример.\n",
    "\n",
    "Инструменты:\n",
    "- sklearn.pipeline.Pipeline\n",
    "- GridSearchCV для подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В следующем блоке содержится код, отрабатывающий процесс от загрузки датасета, до обучения модели и вывода результатов. Без проверок и с меньшим количеством комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Загружаем датасет Titanic\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/rogovich/Data/master/data/titanic/train.csv')\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X = titanic.drop(['Survived', 'PassengerId', 'Ticket', 'Cabin'], axis = 1)\n",
    "y = titanic['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Смотрим распределение пропусков по признакам\n",
    "print(msno.bar(titanic))\n",
    "\n",
    "# Эта визуализация, даёт нам более интуитивное представление о том, где отсутствуют значения\n",
    "msno.matrix(titanic)\n",
    "plt.show()\n",
    "\n",
    "# Выведем корреляционную матрицу между признаками\n",
    "corr_data = titanic.select_dtypes(include=['float64', 'int64']).corr()  # Выбираем столбцы на основе типа данных\n",
    "\n",
    "# Задаём размер матрицы\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "# Поместим созданную выше корреляционную матрицу в функцию sns.heatmap()\n",
    "sns.heatmap(corr_data, annot = True, fmt='.2g', annot_kws={'size':str(7)}, linewidths = .5, cmap= 'coolwarm')\n",
    "\n",
    "# Объединяем признаки с наибольшей корреляцией (Parch и SibSp) в новые признаки\n",
    "for dataset in [X_train, X_test]:\n",
    "    dataset['family_size'] = dataset['Parch'] + dataset['SibSp']  # Общая численность круга семьи на борту\n",
    "    dataset.drop(['Parch', 'SibSp'], axis = 1, inplace = True)\n",
    "    dataset['is_alone'] = 1\n",
    "    dataset['is_alone'] = dataset['family_size'].apply(lambda x: 1 if x >= 1 else 0)  # 0 - был один, 1 - с семьёй\n",
    "\n",
    "# Извлекаем титулы пассажиров и сохраняем их в новый признак, называемый title\n",
    "for dataset in [X_train, X_test]:\n",
    "  dataset['title'] =  dataset['Name'].str.split(\", \", expand = True)[1].str.split(\".\", expand = True)[0]\n",
    "  dataset.drop([\"Name\"], axis = 1, inplace = True)\n",
    "\n",
    "# Смотрим на количество редких титулов и группируем их\n",
    "X_comb = pd.concat([X_train, X_test])\n",
    "rare_titles = (X_comb['title'].value_counts() < 10)\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "    dataset['title'] = dataset['title'].replace({'Miss': 'Mrs'})  # Группировка Mrs и Miss в одну группу\n",
    "    dataset['title'] = dataset['title'].apply(lambda x: 'rare' if rare_titles[x] else x)  # И всех редких в другую\n",
    "\n",
    "# Разделяем признаки по типам данных\n",
    "cat_cols = ['Embarked', 'Sex', 'Pclass', 'title', 'is_alone']\n",
    "num_cols = ['Age', 'Fare', 'family_size']\n",
    "\n",
    "# Функция для заполнения пропусков и кодирования категориальных данных\n",
    "cat_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)),\n",
    "    ('pca', PCA(n_components = 10))  # PCA для уменьшения размерности, после работы OneHotEncoder\n",
    "])\n",
    "\n",
    "# Функция для масштабирования числовых данных\n",
    "num_transformer = Pipeline(steps = [\n",
    "                          ('imputer', KNNImputer(n_neighbors = 5)),\n",
    "                          ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Строим трансформер для всех данных\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Строим модель\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Выполним поиск гиперпараметров с наилучшим результатом\n",
    "num_transformer_dist = {'preprocessor__num__imputer__n_neighbors': [3, 5, 8],\n",
    "                        'preprocessor__num__imputer__add_indicator': [True, False]}\n",
    "\n",
    "cat_transformer_dist = {'preprocessor__cat__imputer__strategy': ['most_frequent', 'constant'],\n",
    "                        'preprocessor__cat__imputer__add_indicator': [True, False],\n",
    "                        'preprocessor__cat__pca__n_components': [3, 6, 10]}\n",
    "\n",
    "random_forest_dist = {'classifier__n_estimators': [50, 100, 150],\n",
    "                      'classifier__max_depth': list(range(2, 7)),\n",
    "                      'classifier__bootstrap': [True, False]}\n",
    "\n",
    "param_dist = {**num_transformer_dist, **cat_transformer_dist, **random_forest_dist}\n",
    "\n",
    "# По заданию, нужно работать с GridSearchCV, но на моей машине на это ушло 21мин 45сек\n",
    "# grid_search = GridSearchCV(clf,\n",
    "#                             param_grid = param_dist,\n",
    "#                             cv = 4)\n",
    "\n",
    "# Работа RandomizedSearchCV с теми же вводными, на моей машине была выполнена за 1мин 18сек, без значимой потери в результатах\n",
    "random_search = RandomizedSearchCV(clf,\n",
    "                                   param_distributions = param_dist,\n",
    "                                   n_iter = 100)\n",
    "\n",
    "# Обучаем модель\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "# Выводим финальные результаты\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В следующем блоке содержится тот же код что и выше, но со всеми комметариями и дополнениями, которые я делал в процессе выполнения задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет Titanic\n",
    "titanic = pd.read_csv('https://raw.githubusercontent.com/rogovich/Data/master/data/titanic/train.csv')\n",
    "\n",
    "# Разделяем данные на обучающую и тестовую выборки\n",
    "X = titanic.drop(['Survived', 'PassengerId', 'Ticket', 'Cabin'], axis = 1)\n",
    "y = titanic['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Смотрим первые строки датафрейма\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете Titanic есть следующие столбцы:\n",
    "\n",
    "Pclass — класс пассажира (1 — высший, 2 — средний, 3 — низший).\n",
    "\n",
    "Name — имя.\n",
    "\n",
    "Sex — пол.\n",
    "\n",
    "Age — возраст.\n",
    "\n",
    "SibSp — количество братьев, сестер, сводных братьев, сводных сестер, супругов на борту Титаника.\n",
    "\n",
    "Parch — количество родителей, детей (в том числе приемных) на борту Титаника.\n",
    "\n",
    "Ticket — номер билета.\n",
    "\n",
    "Fare — плата за проезд.\n",
    "\n",
    "Cabin — каюта.\n",
    "\n",
    "Embarked — порт посадки (C — Шербур, Q — Квинстаун, S — Саутгемптон)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим распределение пропусков по признакам\n",
    "msno.bar(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта визуализация, даёт нам более интуитивное представление о том, где отсутствуют значения\n",
    "msno.matrix(titanic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим типы данных\n",
    "titanic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем корреляционную матрицу между признаками\n",
    "corr_data = titanic.select_dtypes(include = ['float64', 'int64']).corr()  # Выбираем столбцы на основе типа данных\n",
    "\n",
    "# Задаём размер матрицы\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "# Поместим созданную выше корреляционную матрицу в функцию sns.heatmap()\n",
    "sns.heatmap(corr_data, annot = True, fmt = '.2g', annot_kws = {'size':str(7)}, linewidths = .5, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем признаки с наибольшей корреляцией (Parch и SibSp) в новые признаки\n",
    "for dataset in [X_train, X_test]:\n",
    "    dataset['family_size'] = dataset['Parch'] + dataset['SibSp']\n",
    "    dataset.drop(['Parch', 'SibSp'], axis = 1, inplace = True)\n",
    "    dataset['is_alone'] = 1\n",
    "    dataset['is_alone'] = dataset['family_size'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "\n",
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем титулы пассажиров и сохраняем их в новый признак, называемый title\n",
    "for dataset in [X_train, X_test]:\n",
    "  dataset['title'] =  dataset['Name'].str.split(\", \", expand = True)[1].str.split(\".\", expand = True)[0]\n",
    "  dataset.drop([\"Name\"], axis = 1, inplace = True)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим на количество редких титулов. По факту, есть смысл их сгруппировать\n",
    "X_comb = pd.concat([X_train, X_test])\n",
    "rare_titles = (X_comb['title'].value_counts() < 10)\n",
    "rare_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [X_train, X_test]:\n",
    "    dataset['title'] = dataset['title'].replace({'Miss': 'Mrs'})  # Группировка Mrs и Miss в одну группу\n",
    "    dataset['title'] = dataset['title'].apply(lambda x: 'rare' if rare_titles[x] else x)  # И всех редких в другую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем количество титулов\n",
    "X_comb = pd.concat([X_train, X_test])\n",
    "rare_titles = (X_comb['title'].value_counts() < 10)\n",
    "rare_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим типы данных\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем признаки по типам данных\n",
    "cat_cols = ['Embarked', 'Sex', 'Pclass', 'title', 'is_alone']\n",
    "num_cols = ['Age', 'Fare', 'family_size']\n",
    "\n",
    "# Функция для заполнения пропусков и кодирования категориальных данных\n",
    "cat_transformer = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False)),\n",
    "    ('pca', PCA(n_components = 10))  # PCA для уменьшения размерности, после работы OneHotEncoder\n",
    "])\n",
    "\n",
    "# Функция для масштабирования числовых данных\n",
    "num_transformer = Pipeline(steps = [\n",
    "                          ('imputer', KNNImputer(n_neighbors = 5)),\n",
    "                          ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# Строим трансформер для всех данных\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Строим модель и смотрим результаты CV без настройки гиперпараметров\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier())])\n",
    "\n",
    "cross_val_score(clf, X_train, y_train, cv = 5, scoring = \"accuracy\").mean()  # Результат 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполним поиск гиперпараметров с наилучшим результатом\n",
    "num_transformer_dist = {'preprocessor__num__imputer__n_neighbors': [3, 5, 8],\n",
    "                        'preprocessor__num__imputer__add_indicator': [True, False]}\n",
    "\n",
    "cat_transformer_dist = {'preprocessor__cat__imputer__strategy': ['most_frequent', 'constant'],\n",
    "                        'preprocessor__cat__imputer__add_indicator': [True, False],\n",
    "                        'preprocessor__cat__pca__n_components': [3, 6, 10]}\n",
    "\n",
    "random_forest_dist = {'classifier__n_estimators': [50, 100, 150],\n",
    "                      'classifier__max_depth': list(range(2, 7)),\n",
    "                      'classifier__bootstrap': [True, False]}\n",
    "\n",
    "param_dist = {**num_transformer_dist, **cat_transformer_dist, **random_forest_dist}\n",
    "\n",
    "# По заданию, нужно работать с GridSearchCV, но на моей машине на это ушло 21мин 45сек\n",
    "# grid_search = GridSearchCV(clf,\n",
    "#                             param_grid = param_dist,\n",
    "#                             cv = 4)\n",
    "\n",
    "# Работа RandomizedSearchCV на моей машине была выполнена за 1мин 18сек, без значимой потери в результатах\n",
    "random_search = RandomizedSearchCV(clf,\n",
    "                                   param_distributions = param_dist,\n",
    "                                   n_iter = 100)\n",
    "\n",
    "# Обучаем модель\n",
    "# grid_search.fit(X_train, y_train)\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим результат поиска лучших параметров\n",
    "# grid_search.best_params_\n",
    "random_search.best_params_\n",
    "\n",
    "# Результаты работы GridSearchCV:\n",
    "# {'classifier__bootstrap': True,\n",
    "#  'classifier__max_depth': 6,\n",
    "#  'classifier__n_estimators': 50,\n",
    "#  'preprocessor__cat__imputer__add_indicator': True,\n",
    "#  'preprocessor__cat__imputer__strategy': 'constant',\n",
    "#  'preprocessor__cat__pca__n_components': 3,\n",
    "#  'preprocessor__num__imputer__add_indicator': True,\n",
    "#  'preprocessor__num__imputer__n_neighbors': 8}\n",
    "\n",
    "# Результаты работы RandomizedSearchCV:\n",
    "# {'preprocessor__num__imputer__n_neighbors': 5,\n",
    "#  'preprocessor__num__imputer__add_indicator': True,\n",
    "#  'preprocessor__cat__pca__n_components': 3,\n",
    "#  'preprocessor__cat__imputer__strategy': 'most_frequent',\n",
    "#  'preprocessor__cat__imputer__add_indicator': False,\n",
    "#  'classifier__n_estimators': 100,\n",
    "#  'classifier__max_depth': 6,\n",
    "#  'classifier__bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = grid_search.predict(X_test)\n",
    "y_pred = random_search.predict(X_test)\n",
    "\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выводим финальные результаты\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Результаты с параметрами, определёнными после работы GridSearchCV\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.82      0.90      0.86       105\n",
    "#            1       0.84      0.72      0.77        74\n",
    "\n",
    "#     accuracy                           0.83       179\n",
    "#    macro avg       0.83      0.81      0.82       179\n",
    "# weighted avg       0.83      0.83      0.82       179\n",
    "\n",
    "# Результаты с параметрами, определёнными после работы RandomizedSearchCV\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.81      0.90      0.86       105\n",
    "#            1       0.84      0.70      0.76        74\n",
    "\n",
    "#     accuracy                           0.82       179\n",
    "#    macro avg       0.83      0.80      0.81       179\n",
    "# weighted avg       0.82      0.82      0.82       179"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
