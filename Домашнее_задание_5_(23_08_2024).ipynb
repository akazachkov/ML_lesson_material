{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание № 5\n",
    "\n",
    "Цель:\n",
    "\n",
    "Разработать классификатор для определения спам/не \tспам сообщений с использованием различных методов предобработки текста и векторизации.\n",
    "\n",
    "Задания: \n",
    "\n",
    "1. Подготовка данных:\n",
    "- Загрузите датасет SMS сообщений, размеченных как спам или не спам.\n",
    "\n",
    "https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
    "\n",
    "2. Предобработка текста:\n",
    "- Реализуйте лемматизацию или стемминг для текстов сообщений.\n",
    "\n",
    "3. Векторизация текста:\n",
    "- Примените мешок слов (Bag of Words) и TF-IDF для векторизации текста.\n",
    "\n",
    "4. Моделирование:\n",
    "- Постройте модели для классификации сообщений как спам или не спам (модели на ваш выбор).\n",
    "- Сравните результаты моделей с использованием различных методов векторизации и предобработки текста.\n",
    "\n",
    "5. Сравнительный анализ:\n",
    "- Оцените качество моделей с различными комбинациями предобработки и векторизации (сравните метрики ROC-AUC, F1-score, accuracy и т.д.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем всё необходимое для работы\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import metrics\n",
    "\n",
    "# Строим загрузчик датасета, конвееры-предобработчики текста, преобразователи в матрицу, модели и сборщик метрик\n",
    "\n",
    "def load_set_and_preclean(df):\n",
    "    '''Задачи функции:\n",
    "    1) Загрузка датасета;\n",
    "    2) Сохранение признаков для дальнейшей работы;\n",
    "    3) Кодирование данных в целевом признаке.\n",
    "    '''\n",
    "    # Загружаем SMS Spam Collection Dataset\n",
    "    df = pd.read_csv(\n",
    "        'https://raw.githubusercontent.com/akazachkov/ML_lesson_material/main/material_for_work/spam.csv',\n",
    "        encoding = 'latin-1'\n",
    "        )  # latin-1 используется, потому что UTF-8 (по умолчанию) вызовет ошибку\n",
    "\n",
    "    df = df[['v1','v2']]  # Сохраняем только первые два признака\n",
    "    df.columns = ['Target', 'Text']  # переименовываем их для удобства\n",
    "\n",
    "    # Кодируем категориальные данные в числовые\n",
    "    label_encoder = LabelEncoder()\n",
    "    df[\"Target\"] = label_encoder.fit_transform(df[\"Target\"])\n",
    "    return df\n",
    "\n",
    "def preprocess_stemmer(text):\n",
    "    '''\n",
    "    Задачи, выполняемые обработчиком:\n",
    "    1) Приведение текста к нижнему регистру;\n",
    "    2) Исключение знаков препинания;\n",
    "    3) Исключение стоп-слова;\n",
    "    4) Стемминг.\n",
    "    '''\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
    "    lemmatized_output = ' '.join([stemmer.stem(word) for word in filtered_tokens])\n",
    "    return lemmatized_output\n",
    "\n",
    "def preprocess_lemmatizer(text):\n",
    "    '''\n",
    "    Задачи, выполняемые обработчиком:\n",
    "    1) Приведение текста к нижнему регистру;\n",
    "    2) Исключение знаков препинания;\n",
    "    3) Исключение стоп-слова;\n",
    "    4) Лемматизация.\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word, pos ='v') for word in filtered_tokens])\n",
    "    return lemmatized_output\n",
    "\n",
    "def train_test_split_df(df):\n",
    "    '''Производим деление датафрейм на тренировочный и тестовый сеты, сбрасываем индексы'''\n",
    "    train, test = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "    train.reset_index(inplace = True)\n",
    "    test.reset_index(inplace = True)\n",
    "    return (train, test)\n",
    "\n",
    "def TfidfVector(train, test):\n",
    "    '''Преобразуем текстовые данные в матрицу TF-IDF с определёнными гиперпараметрами'''\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df = 8,  # Определяем минимальную частоту документов, в которых должен встречаться термин\n",
    "        max_df = 0.4,  # Определяем максимальную долю документов, в которых может встречаться термин\n",
    "        ngram_range = (1, 2)  # Указываем, чтобы в словаре появились биграммы\n",
    "        )\n",
    "    X_train = vectorizer.fit_transform(train.Text)\n",
    "    X_test = vectorizer.transform(test.Text)\n",
    "    y_train = train.Target.values\n",
    "    y_test = test.Target.values\n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "def CountVector(train, test):\n",
    "    '''В этой функции вместо TF-IDF используем мешок слов (Bag of words) с такими же гиперпараметрами'''\n",
    "    vectorizer = CountVectorizer(min_df = 8, max_df = 0.4, ngram_range = (1, 2))\n",
    "    X_train = vectorizer.fit_transform(train.Text)\n",
    "    X_test = vectorizer.transform(test.Text)\n",
    "    y_train = train.Target.values\n",
    "    y_test = test.Target.values\n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "def models_building_and_evaluation(X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Задачи, выполняемые функцией:\n",
    "    1) Создание моделей с определёнными параметрами;\n",
    "    2) Обучение моделей;\n",
    "    3) Оценка моделей и сохранение результатов в словарь;\n",
    "    4) Добавление данных из словаря в DataFrame и вывод сводной информации.\n",
    "    '''\n",
    "    # Создаём листы, куда будем сохранять метрики\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    roc_auc_score = []\n",
    "    train_set_accuracy = []\n",
    "    test_set_accuracy = []\n",
    "\n",
    "    # Строим модели и задаём гиперпараметры\n",
    "    classifiers = [MultinomialNB(alpha = 1.),\n",
    "                   DecisionTreeClassifier(max_depth = 10),\n",
    "                   RandomForestClassifier(n_estimators = 100, max_depth = 9, max_features = None),\n",
    "                   KNeighborsClassifier(n_neighbors = 10, metric = 'cosine')]\n",
    "\n",
    "    # Обучаем модели\n",
    "    for cls in classifiers:\n",
    "        cls.fit(X_train, y_train)\n",
    "\n",
    "    # Оцениваем качество моделей\n",
    "    for i in classifiers:\n",
    "        pred_train = i.predict(X_train)\n",
    "        pred_test = i.predict(X_test)\n",
    "        precis = metrics.precision_score(y_test, pred_test)\n",
    "        recal = metrics.recall_score(y_test, pred_test)\n",
    "        f1_sc = metrics.f1_score(y_test, pred_test)\n",
    "        roc_auc = metrics.roc_auc_score(y_test, pred_test)\n",
    "        train_accuracy = metrics.accuracy_score(y_train, pred_train)\n",
    "        test_accuracy = metrics.accuracy_score(y_test, pred_test)\n",
    "\n",
    "        # Сохраняем метрики\n",
    "        precision.append(precis)\n",
    "        recall.append(recal)\n",
    "        f1_score.append(f1_sc)\n",
    "        roc_auc_score.append(roc_auc)\n",
    "        train_set_accuracy.append(train_accuracy)\n",
    "        test_set_accuracy.append(test_accuracy)\n",
    "\n",
    "    # Собираем все метрики в один словарь\n",
    "    data_results = {'Precision': precision,\n",
    "                    'Recall': recall,\n",
    "                    'F1_score': f1_score,\n",
    "                    'ROC AUC': roc_auc_score,\n",
    "                    'Accuracy on test set': test_set_accuracy,\n",
    "                    'Accuracy on train set': train_set_accuracy}\n",
    "\n",
    "    # Добавляем данные из словаря data_results в pandas DataFrame и выводим сводную информацию\n",
    "    data_results_pd = pd.DataFrame(data_results, index = [\"NaiveBayes\",\n",
    "                                                          \"DecisionTreeClassifier\",\n",
    "                                                          \"RandomForestClassifier\",\n",
    "                                                          \"KNeighbours\"])\n",
    "    return data_results_pd.style.background_gradient(cmap = ListedColormap([\"#fffaf0\", \"#fbceb1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cf856_row0_col0, #T_cf856_row1_col1, #T_cf856_row1_col2, #T_cf856_row1_col3, #T_cf856_row1_col4, #T_cf856_row1_col5, #T_cf856_row2_col1, #T_cf856_row2_col2, #T_cf856_row2_col3, #T_cf856_row2_col4, #T_cf856_row2_col5, #T_cf856_row3_col0, #T_cf856_row3_col1, #T_cf856_row3_col2, #T_cf856_row3_col3, #T_cf856_row3_col4, #T_cf856_row3_col5 {\n",
       "  background-color: #fffaf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_cf856_row0_col1, #T_cf856_row0_col2, #T_cf856_row0_col3, #T_cf856_row0_col4, #T_cf856_row0_col5, #T_cf856_row1_col0, #T_cf856_row2_col0 {\n",
       "  background-color: #fbceb1;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cf856\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cf856_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_cf856_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_cf856_level0_col2\" class=\"col_heading level0 col2\" >F1_score</th>\n",
       "      <th id=\"T_cf856_level0_col3\" class=\"col_heading level0 col3\" >ROC AUC</th>\n",
       "      <th id=\"T_cf856_level0_col4\" class=\"col_heading level0 col4\" >Accuracy on test set</th>\n",
       "      <th id=\"T_cf856_level0_col5\" class=\"col_heading level0 col5\" >Accuracy on train set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cf856_level0_row0\" class=\"row_heading level0 row0\" >NaiveBayes</th>\n",
       "      <td id=\"T_cf856_row0_col0\" class=\"data row0 col0\" >0.909091</td>\n",
       "      <td id=\"T_cf856_row0_col1\" class=\"data row0 col1\" >0.866667</td>\n",
       "      <td id=\"T_cf856_row0_col2\" class=\"data row0 col2\" >0.887372</td>\n",
       "      <td id=\"T_cf856_row0_col3\" class=\"data row0 col3\" >0.926598</td>\n",
       "      <td id=\"T_cf856_row0_col4\" class=\"data row0 col4\" >0.970404</td>\n",
       "      <td id=\"T_cf856_row0_col5\" class=\"data row0 col5\" >0.982051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cf856_level0_row1\" class=\"row_heading level0 row1\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_cf856_row1_col0\" class=\"data row1 col0\" >0.949495</td>\n",
       "      <td id=\"T_cf856_row1_col1\" class=\"data row1 col1\" >0.626667</td>\n",
       "      <td id=\"T_cf856_row1_col2\" class=\"data row1 col2\" >0.755020</td>\n",
       "      <td id=\"T_cf856_row1_col3\" class=\"data row1 col3\" >0.810743</td>\n",
       "      <td id=\"T_cf856_row1_col4\" class=\"data row1 col4\" >0.945291</td>\n",
       "      <td id=\"T_cf856_row1_col5\" class=\"data row1 col5\" >0.963428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cf856_level0_row2\" class=\"row_heading level0 row2\" >RandomForestClassifier</th>\n",
       "      <td id=\"T_cf856_row2_col0\" class=\"data row2 col0\" >0.980000</td>\n",
       "      <td id=\"T_cf856_row2_col1\" class=\"data row2 col1\" >0.653333</td>\n",
       "      <td id=\"T_cf856_row2_col2\" class=\"data row2 col2\" >0.784000</td>\n",
       "      <td id=\"T_cf856_row2_col3\" class=\"data row2 col3\" >0.825630</td>\n",
       "      <td id=\"T_cf856_row2_col4\" class=\"data row2 col4\" >0.951570</td>\n",
       "      <td id=\"T_cf856_row2_col5\" class=\"data row2 col5\" >0.969037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cf856_level0_row3\" class=\"row_heading level0 row3\" >KNeighbours</th>\n",
       "      <td id=\"T_cf856_row3_col0\" class=\"data row3 col0\" >0.936364</td>\n",
       "      <td id=\"T_cf856_row3_col1\" class=\"data row3 col1\" >0.686667</td>\n",
       "      <td id=\"T_cf856_row3_col2\" class=\"data row3 col2\" >0.792308</td>\n",
       "      <td id=\"T_cf856_row3_col3\" class=\"data row3 col3\" >0.839706</td>\n",
       "      <td id=\"T_cf856_row3_col4\" class=\"data row3 col4\" >0.951570</td>\n",
       "      <td id=\"T_cf856_row3_col5\" class=\"data row3 col5\" >0.961409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a7feca4c20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Проходим весь процесс от загрузки датасета до вывода результатов.\n",
    "Смотрим на результаты работы сочетания лемматизации и \"мешка слов\"\n",
    "'''\n",
    "\n",
    "# Создаём пустую переменную, куда будет загружен датасет\n",
    "new_df = []\n",
    "\n",
    "# Загружаем датасет и предварительно его обрабатываем\n",
    "sms_df = load_set_and_preclean(new_df)\n",
    "\n",
    "# Используем предобработчик текста, дополненный лемматизацией\n",
    "sms_df['Text'] = sms_df['Text'].apply(preprocess_lemmatizer)\n",
    "\n",
    "# Делим датафрейм на тренировочный и тестовый сеты\n",
    "train, test = train_test_split_df(sms_df)\n",
    "\n",
    "# Преобразуем текстовые данные с помощью техники \"мешок слов\"\n",
    "X_train, X_test, y_train, y_test = CountVector(train, test)\n",
    "\n",
    "# Создаём модели, обучаем и выводим результаты\n",
    "data_results_lem_countvec = models_building_and_evaluation(X_train, X_test, y_train, y_test)\n",
    "data_results_lem_countvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c0b4d_row0_col0, #T_c0b4d_row0_col1, #T_c0b4d_row0_col2, #T_c0b4d_row0_col3, #T_c0b4d_row0_col4, #T_c0b4d_row0_col5, #T_c0b4d_row1_col5, #T_c0b4d_row2_col5, #T_c0b4d_row3_col0 {\n",
       "  background-color: #fbceb1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b4d_row1_col0, #T_c0b4d_row1_col1, #T_c0b4d_row1_col2, #T_c0b4d_row1_col3, #T_c0b4d_row1_col4, #T_c0b4d_row2_col0, #T_c0b4d_row2_col1, #T_c0b4d_row2_col2, #T_c0b4d_row2_col3, #T_c0b4d_row2_col4, #T_c0b4d_row3_col1, #T_c0b4d_row3_col2, #T_c0b4d_row3_col3, #T_c0b4d_row3_col4, #T_c0b4d_row3_col5 {\n",
       "  background-color: #fffaf0;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c0b4d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c0b4d_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_c0b4d_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_c0b4d_level0_col2\" class=\"col_heading level0 col2\" >F1_score</th>\n",
       "      <th id=\"T_c0b4d_level0_col3\" class=\"col_heading level0 col3\" >ROC AUC</th>\n",
       "      <th id=\"T_c0b4d_level0_col4\" class=\"col_heading level0 col4\" >Accuracy on test set</th>\n",
       "      <th id=\"T_c0b4d_level0_col5\" class=\"col_heading level0 col5\" >Accuracy on train set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b4d_level0_row0\" class=\"row_heading level0 row0\" >NaiveBayes</th>\n",
       "      <td id=\"T_c0b4d_row0_col0\" class=\"data row0 col0\" >0.967480</td>\n",
       "      <td id=\"T_c0b4d_row0_col1\" class=\"data row0 col1\" >0.793333</td>\n",
       "      <td id=\"T_c0b4d_row0_col2\" class=\"data row0 col2\" >0.871795</td>\n",
       "      <td id=\"T_c0b4d_row0_col3\" class=\"data row0 col3\" >0.894594</td>\n",
       "      <td id=\"T_c0b4d_row0_col4\" class=\"data row0 col4\" >0.968610</td>\n",
       "      <td id=\"T_c0b4d_row0_col5\" class=\"data row0 col5\" >0.980256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b4d_level0_row1\" class=\"row_heading level0 row1\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_c0b4d_row1_col0\" class=\"data row1 col0\" >0.900826</td>\n",
       "      <td id=\"T_c0b4d_row1_col1\" class=\"data row1 col1\" >0.726667</td>\n",
       "      <td id=\"T_c0b4d_row1_col2\" class=\"data row1 col2\" >0.804428</td>\n",
       "      <td id=\"T_c0b4d_row1_col3\" class=\"data row1 col3\" >0.857116</td>\n",
       "      <td id=\"T_c0b4d_row1_col4\" class=\"data row1 col4\" >0.952466</td>\n",
       "      <td id=\"T_c0b4d_row1_col5\" class=\"data row1 col5\" >0.975768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b4d_level0_row2\" class=\"row_heading level0 row2\" >RandomForestClassifier</th>\n",
       "      <td id=\"T_c0b4d_row2_col0\" class=\"data row2 col0\" >0.903226</td>\n",
       "      <td id=\"T_c0b4d_row2_col1\" class=\"data row2 col1\" >0.746667</td>\n",
       "      <td id=\"T_c0b4d_row2_col2\" class=\"data row2 col2\" >0.817518</td>\n",
       "      <td id=\"T_c0b4d_row2_col3\" class=\"data row2 col3\" >0.867116</td>\n",
       "      <td id=\"T_c0b4d_row2_col4\" class=\"data row2 col4\" >0.955157</td>\n",
       "      <td id=\"T_c0b4d_row2_col5\" class=\"data row2 col5\" >0.975544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b4d_level0_row3\" class=\"row_heading level0 row3\" >KNeighbours</th>\n",
       "      <td id=\"T_c0b4d_row3_col0\" class=\"data row3 col0\" >0.955752</td>\n",
       "      <td id=\"T_c0b4d_row3_col1\" class=\"data row3 col1\" >0.720000</td>\n",
       "      <td id=\"T_c0b4d_row3_col2\" class=\"data row3 col2\" >0.821293</td>\n",
       "      <td id=\"T_c0b4d_row3_col3\" class=\"data row3 col3\" >0.857409</td>\n",
       "      <td id=\"T_c0b4d_row3_col4\" class=\"data row3 col4\" >0.957848</td>\n",
       "      <td id=\"T_c0b4d_row3_col5\" class=\"data row3 col5\" >0.965672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a781055550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Проходим весь процесс от загрузки датасета до вывода результатов.\n",
    "Смотрим на результаты работы сочетания стемминга и TF-IDF\n",
    "'''\n",
    "\n",
    "# Создаём пустую переменную, куда будет загружен датасет\n",
    "new_df = []\n",
    "\n",
    "# Загружаем датасет и предварительно его обрабатываем\n",
    "sms_df = load_set_and_preclean(new_df)\n",
    "\n",
    "# Используем предобработчик текста, дополненный стеммингом\n",
    "sms_df['Text'] = sms_df['Text'].apply(preprocess_stemmer)\n",
    "\n",
    "# Делим датафрейм на тренировочный и тестовый сеты\n",
    "train, test = train_test_split_df(sms_df)\n",
    "\n",
    "# Преобразуем текстовые данные в матрицу TF-IDF\n",
    "X_train, X_test, y_train, y_test = TfidfVector(train, test)\n",
    "\n",
    "# Создаём модели, обучаем и выводим результаты\n",
    "data_results_stem_tf = models_building_and_evaluation(X_train, X_test, y_train, y_test)\n",
    "data_results_stem_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3aa0d_row0_col0, #T_3aa0d_row0_col1, #T_3aa0d_row0_col2, #T_3aa0d_row0_col3, #T_3aa0d_row0_col4, #T_3aa0d_row0_col5, #T_3aa0d_row1_col5, #T_3aa0d_row2_col5, #T_3aa0d_row3_col0 {\n",
       "  background-color: #fbceb1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3aa0d_row1_col0, #T_3aa0d_row1_col1, #T_3aa0d_row1_col2, #T_3aa0d_row1_col3, #T_3aa0d_row1_col4, #T_3aa0d_row2_col0, #T_3aa0d_row2_col1, #T_3aa0d_row2_col2, #T_3aa0d_row2_col3, #T_3aa0d_row2_col4, #T_3aa0d_row3_col1, #T_3aa0d_row3_col2, #T_3aa0d_row3_col3, #T_3aa0d_row3_col4, #T_3aa0d_row3_col5 {\n",
       "  background-color: #fffaf0;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3aa0d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3aa0d_level0_col0\" class=\"col_heading level0 col0\" >Precision</th>\n",
       "      <th id=\"T_3aa0d_level0_col1\" class=\"col_heading level0 col1\" >Recall</th>\n",
       "      <th id=\"T_3aa0d_level0_col2\" class=\"col_heading level0 col2\" >F1_score</th>\n",
       "      <th id=\"T_3aa0d_level0_col3\" class=\"col_heading level0 col3\" >ROC AUC</th>\n",
       "      <th id=\"T_3aa0d_level0_col4\" class=\"col_heading level0 col4\" >Accuracy on test set</th>\n",
       "      <th id=\"T_3aa0d_level0_col5\" class=\"col_heading level0 col5\" >Accuracy on train set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3aa0d_level0_row0\" class=\"row_heading level0 row0\" >NaiveBayes</th>\n",
       "      <td id=\"T_3aa0d_row0_col0\" class=\"data row0 col0\" >0.967480</td>\n",
       "      <td id=\"T_3aa0d_row0_col1\" class=\"data row0 col1\" >0.793333</td>\n",
       "      <td id=\"T_3aa0d_row0_col2\" class=\"data row0 col2\" >0.871795</td>\n",
       "      <td id=\"T_3aa0d_row0_col3\" class=\"data row0 col3\" >0.894594</td>\n",
       "      <td id=\"T_3aa0d_row0_col4\" class=\"data row0 col4\" >0.968610</td>\n",
       "      <td id=\"T_3aa0d_row0_col5\" class=\"data row0 col5\" >0.980031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3aa0d_level0_row1\" class=\"row_heading level0 row1\" >DecisionTreeClassifier</th>\n",
       "      <td id=\"T_3aa0d_row1_col0\" class=\"data row1 col0\" >0.885246</td>\n",
       "      <td id=\"T_3aa0d_row1_col1\" class=\"data row1 col1\" >0.720000</td>\n",
       "      <td id=\"T_3aa0d_row1_col2\" class=\"data row1 col2\" >0.794118</td>\n",
       "      <td id=\"T_3aa0d_row1_col3\" class=\"data row1 col3\" >0.852746</td>\n",
       "      <td id=\"T_3aa0d_row1_col4\" class=\"data row1 col4\" >0.949776</td>\n",
       "      <td id=\"T_3aa0d_row1_col5\" class=\"data row1 col5\" >0.975544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3aa0d_level0_row2\" class=\"row_heading level0 row2\" >RandomForestClassifier</th>\n",
       "      <td id=\"T_3aa0d_row2_col0\" class=\"data row2 col0\" >0.910569</td>\n",
       "      <td id=\"T_3aa0d_row2_col1\" class=\"data row2 col1\" >0.746667</td>\n",
       "      <td id=\"T_3aa0d_row2_col2\" class=\"data row2 col2\" >0.820513</td>\n",
       "      <td id=\"T_3aa0d_row2_col3\" class=\"data row2 col3\" >0.867634</td>\n",
       "      <td id=\"T_3aa0d_row2_col4\" class=\"data row2 col4\" >0.956054</td>\n",
       "      <td id=\"T_3aa0d_row2_col5\" class=\"data row2 col5\" >0.975544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3aa0d_level0_row3\" class=\"row_heading level0 row3\" >KNeighbours</th>\n",
       "      <td id=\"T_3aa0d_row3_col0\" class=\"data row3 col0\" >0.955752</td>\n",
       "      <td id=\"T_3aa0d_row3_col1\" class=\"data row3 col1\" >0.720000</td>\n",
       "      <td id=\"T_3aa0d_row3_col2\" class=\"data row3 col2\" >0.821293</td>\n",
       "      <td id=\"T_3aa0d_row3_col3\" class=\"data row3 col3\" >0.857409</td>\n",
       "      <td id=\"T_3aa0d_row3_col4\" class=\"data row3 col4\" >0.957848</td>\n",
       "      <td id=\"T_3aa0d_row3_col5\" class=\"data row3 col5\" >0.964999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a7fea6eab0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Проходим весь процесс от загрузки датасета до вывода результатов.\n",
    "Смотрим на результаты работы сочетания лемматизации и TF-IDF\n",
    "'''\n",
    "\n",
    "# Создаём пустую переменную, куда будет загружен датасет\n",
    "new_df = []\n",
    "\n",
    "# Загружаем датасет и предварительно его обрабатываем\n",
    "sms_df = load_set_and_preclean(new_df)\n",
    "\n",
    "# Используем предобработчик текста, дополненный лемматизацией\n",
    "sms_df['Text'] = sms_df['Text'].apply(preprocess_lemmatizer)\n",
    "\n",
    "# Делим датафрейм на тренировочный и тестовый сеты\n",
    "train, test = train_test_split_df(sms_df)\n",
    "\n",
    "# Преобразуем текстовые данные в матрицу TF-IDF\n",
    "X_train, X_test, y_train, y_test = TfidfVector(train, test)\n",
    "\n",
    "# Создаём модели, обучаем и выводим результаты\n",
    "data_results_lem_tf = models_building_and_evaluation(X_train, X_test, y_train, y_test)\n",
    "data_results_lem_tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
